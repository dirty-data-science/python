
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gen_notes/01_missing_values.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gen_notes_01_missing_values.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gen_notes_01_missing_values.py:


=========================================
Machine learning with missing values
=========================================

Here we use simulated data to understanding the fundamentals of statistical
learning with missing values.

This notebook reveals why a HistGradientBoostingRegressor (
:class:`sklearn.ensemble.HistGradientBoostingRegressor` ) is a choice to
predict with missing values.

We use simulations to control the missing-value mechanism, and inspect
it's impact on predictive models. In particular, standard imputation
procedures can reconstruct missing values without distortion only if the
data is *missing at random*.

The mathematical details behind this notebook can be found in
https://arxiv.org/abs/1902.06931

.. GENERATED FROM PYTHON SOURCE LINES 24-33

The fully-observed data: a toy regression problem
==================================================

We consider a simple regression problem where X (the data) is bivariate
gaussian, and y (the prediction target)  is a linear function of the first
coordinate, with noise.

The data-generating mechanism
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 33-48

.. code-block:: default


    import numpy as np

    def generate_without_missing_values(n_samples, rng=42):
        mean = [0, 0]
        cov = [[1, 0.5], [0.5, 1]]
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)
        X = rng.multivariate_normal(mean, cov, size=n_samples)

        epsilon = 0.1 * rng.randn(n_samples)
        y = X[:, 0] + epsilon

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 49-50

A quick plot reveals what the data looks like

.. GENERATED FROM PYTHON SOURCE LINES 50-59

.. code-block:: default


    import matplotlib.pyplot as plt
    plt.rcParams['figure.figsize'] = (5, 4) # Smaller default figure size

    plt.figure()
    X_full, y_full = generate_without_missing_values(1000)
    plt.scatter(X_full[:, 0], X_full[:, 1], c=y_full)
    plt.colorbar(label='y')




.. image:: /gen_notes/images/sphx_glr_01_missing_values_001.png
    :alt: 01 missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7f5039a4fb20>



.. GENERATED FROM PYTHON SOURCE LINES 60-69

Missing completely at random settings
======================================

We now consider missing completely at random settings (a special case
of missing at random): the missingness is completely independent from
the values.

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 69-80

.. code-block:: default


    def generate_mcar(n_samples, missing_rate=.5, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        M = rng.binomial(1, missing_rate, (n_samples, 2))
        np.putmask(X, M, np.nan)

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 81-82

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 82-90

.. code-block:: default

    X, y = generate_mcar(500)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5', label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_01_missing_values_002.png
    :alt: 01 missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f50398b3f40>



.. GENERATED FROM PYTHON SOURCE LINES 91-102

We can see that the distribution of the fully-observed data is the same
than that of the original data

Conditional Imputation with the IterativeImputer
------------------------------------------------

As the data is MAR (missing at random), an imputer can use the
conditional dependencies between the observed and the missing values to
impute the missing values.

We'll use the IterativeImputer, a good imputer, but it needs to be enabled

.. GENERATED FROM PYTHON SOURCE LINES 102-106

.. code-block:: default

    from sklearn.experimental import enable_iterative_imputer
    from sklearn import impute
    iterative_imputer = impute.IterativeImputer()








.. GENERATED FROM PYTHON SOURCE LINES 107-111

Let us try the imputer on the small data used to visualize

**The imputation is learned by fitting the imputer on the data with
missing values**

.. GENERATED FROM PYTHON SOURCE LINES 111-113

.. code-block:: default

    iterative_imputer.fit(X)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    IterativeImputer()



.. GENERATED FROM PYTHON SOURCE LINES 114-115

**The data are imputed with the transform method**

.. GENERATED FROM PYTHON SOURCE LINES 115-117

.. code-block:: default

    X_imputed = iterative_imputer.transform(X)








.. GENERATED FROM PYTHON SOURCE LINES 118-119

We can display the imputed data as our previous visualization

.. GENERATED FROM PYTHON SOURCE LINES 119-127

.. code-block:: default

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data', alpha=.5)
    plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',
                label='Imputed')
    plt.colorbar(label='y')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_01_missing_values_003.png
    :alt: 01 missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f500f3157f0>



.. GENERATED FROM PYTHON SOURCE LINES 128-140

We can see that the imputer did a fairly good job of recovering the
data distribution

Supervised learning: imputation and a linear model
-----------------------------------------------------------

Given that the relationship between the fully-observed X and y is a
linear relationship, it seems natural to use a linear model for
prediction. It must be adapted to missing values using imputation.

To use it in supervised setting, we will pipeline it with a linear
model, using a ridge, which is a good default linear model

.. GENERATED FROM PYTHON SOURCE LINES 140-145

.. code-block:: default

    from sklearn.pipeline import make_pipeline
    from sklearn.linear_model import RidgeCV

    iterative_and_ridge = make_pipeline(impute.IterativeImputer(), RidgeCV())








.. GENERATED FROM PYTHON SOURCE LINES 146-149

We can evaluate the model performance in a cross-validation loop
(for better evaluation accuracy, we increase slightly the number of
folds to 10)

.. GENERATED FROM PYTHON SOURCE LINES 149-155

.. code-block:: default

    from sklearn import model_selection
    scores_iterative_and_ridge = model_selection.cross_val_score(
        iterative_and_ridge, X, y, cv=10)

    scores_iterative_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.46386365, 0.31905505, 0.62796809, 0.50172353, 0.5526797 ,
           0.52067976, 0.66790152, 0.71185873, 0.52386641, 0.45218862])



.. GENERATED FROM PYTHON SOURCE LINES 156-159

**Computational cost**: One drawback of the IterativeImputer to keep in
mind is that its computational cost can become prohibitive of large
datasets (it has a bad computation scalability).

.. GENERATED FROM PYTHON SOURCE LINES 161-165

Mean imputation: SimpleImputer
-------------------------------

We can try a simple imputer: imputation by the mean

.. GENERATED FROM PYTHON SOURCE LINES 165-167

.. code-block:: default

    mean_imputer = impute.SimpleImputer()








.. GENERATED FROM PYTHON SOURCE LINES 168-169

A quick visualization reveals a larger disortion of the distribution

.. GENERATED FROM PYTHON SOURCE LINES 169-177

.. code-block:: default

    X_imputed = mean_imputer.fit_transform(X)
    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data', alpha=.5)
    plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',
                label='Imputed')
    plt.colorbar(label='y')




.. image:: /gen_notes/images/sphx_glr_01_missing_values_004.png
    :alt: 01 missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7f500f2a2370>



.. GENERATED FROM PYTHON SOURCE LINES 178-179

Evaluating in prediction pipeline

.. GENERATED FROM PYTHON SOURCE LINES 179-185

.. code-block:: default

    mean_and_ridge = make_pipeline(impute.SimpleImputer(), RidgeCV())
    scores_mean_and_ridge = model_selection.cross_val_score(
        mean_and_ridge, X, y, cv=10)

    scores_mean_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.54614659, 0.43926104, 0.59187943, 0.44029222, 0.53507246,
           0.41132337, 0.67374847, 0.79018856, 0.48898176, 0.46854395])



.. GENERATED FROM PYTHON SOURCE LINES 186-191

Supervised learning without imputation
----------------------------------------

The HistGradientBoosting models are based on trees, which can be
adapted to model directly missing values

.. GENERATED FROM PYTHON SOURCE LINES 191-198

.. code-block:: default

    from sklearn.experimental import enable_hist_gradient_boosting
    from sklearn.ensemble import HistGradientBoostingRegressor
    score_hist_gradient_boosting = model_selection.cross_val_score(
        HistGradientBoostingRegressor(), X, y, cv=10)

    score_hist_gradient_boosting





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/varoquau/dev/scikit-learn/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.
      warnings.warn(

    array([0.48104853, 0.3676542 , 0.54779672, 0.43052366, 0.53230889,
           0.31475748, 0.66549077, 0.70964002, 0.5001503 , 0.43511816])



.. GENERATED FROM PYTHON SOURCE LINES 199-203

Recap: which pipeline predicts well on our small data?
-------------------------------------------------------

Let's plot the scores to see things better

.. GENERATED FROM PYTHON SOURCE LINES 203-217

.. code-block:: default

    import pandas as pd
    import seaborn as sns

    scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,
                 'IterativeImputer + Ridge': scores_iterative_and_ridge,
                 'HistGradientBoostingRegressor': score_hist_gradient_boosting,
        })

    sns.boxplot(data=scores, orient='h')
    plt.title('Prediction accuracy\n linear and small data\n'
              'Missing Completely at Random')
    plt.tight_layout()





.. image:: /gen_notes/images/sphx_glr_01_missing_values_005.png
    :alt: Prediction accuracy  linear and small data Missing Completely at Random
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 218-225

Not much difference with the more sophisticated imputer. A more thorough
analysis would be necessary, with more cross-validation runs.

Prediction performance with large datasets
-------------------------------------------

Let us compare models in regimes where there is plenty of data

.. GENERATED FROM PYTHON SOURCE LINES 225-228

.. code-block:: default


    X, y = generate_mcar(n_samples=20000)








.. GENERATED FROM PYTHON SOURCE LINES 229-230

Iterative imputation and linear model

.. GENERATED FROM PYTHON SOURCE LINES 230-233

.. code-block:: default

    scores_iterative_and_ridge= model_selection.cross_val_score(
        iterative_and_ridge, X, y, cv=10)








.. GENERATED FROM PYTHON SOURCE LINES 234-235

Mean imputation and linear model

.. GENERATED FROM PYTHON SOURCE LINES 235-238

.. code-block:: default

    scores_mean_and_ridge = model_selection.cross_val_score(
        mean_and_ridge, X, y, cv=10)








.. GENERATED FROM PYTHON SOURCE LINES 239-241

And now the HistGradientBoostingRegressor, which does not need
imputation

.. GENERATED FROM PYTHON SOURCE LINES 241-244

.. code-block:: default

    score_hist_gradient_boosting = model_selection.cross_val_score(
        HistGradientBoostingRegressor(), X, y, cv=10)








.. GENERATED FROM PYTHON SOURCE LINES 245-246

We plot the results

.. GENERATED FROM PYTHON SOURCE LINES 246-257

.. code-block:: default

    scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,
                 'IterativeImputer + Ridge': scores_iterative_and_ridge,
                 'HistGradientBoostingRegressor': score_hist_gradient_boosting,
        })

    sns.boxplot(data=scores, orient='h')
    plt.title('Prediction accuracy\n linear and large data\n'
              'Missing Completely at Random')
    plt.tight_layout()





.. image:: /gen_notes/images/sphx_glr_01_missing_values_006.png
    :alt: Prediction accuracy  linear and large data Missing Completely at Random
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 258-271

**When there is a reasonnable amout of data, the
HistGradientBoostingRegressor is the best strategy** even for a linear
data-generating mechanism, in MAR settings, which are settings
favorable to imputation + linear model [#]_.

.. [#] Even in the case of a linear data-generating mechanism, the
       optimal prediction one data imputed by a constant
       is a piecewise affine function with 2^d regions (
       http://proceedings.mlr.press/v108/morvan20a.html ). The
       larger the dimensionality (number of features), the more a
       imperfect imputation is hard to approximate with a simple model.

|

.. GENERATED FROM PYTHON SOURCE LINES 274-283

Missing not at random: censoring
======================================

We now consider missing not at random settings, in particular
self-masking or censoring, where large values are more likely to be
missing.

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 283-296

.. code-block:: default


    def generate_censored(n_samples, missing_rate=.4, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        B = rng.binomial(1, 2 * missing_rate, (n_samples, 2))
        M = (X > 0.5) * B

        np.putmask(X, M, np.nan)

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 297-298

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 298-307

.. code-block:: default

    X, y = generate_censored(500, missing_rate=.4)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_01_missing_values_007.png
    :alt: 01 missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f50067cddc0>



.. GENERATED FROM PYTHON SOURCE LINES 308-310

Does the full-observed data reflect well the distribution of all the
data?

.. GENERATED FROM PYTHON SOURCE LINES 312-316

Imputation
-----------

Use an iterative imputer to create a completed data set. Visualize it

.. GENERATED FROM PYTHON SOURCE LINES 319-331

Does the imputed data reflect well the initial data distribution? What
are the important difference?

An important consequence is that **the link between imputed X and y is no
longer linear**, although the original data-generating mechanism is
linear [#]_. For this reason, **it is often a good idea to use non-linear
learners in the presence of missing values**.

.. [#] As mentionned above, even in the case of a linear
   data-generating mechanism, imperfect imputation leads to complex
   functions to link to y (
   http://proceedings.mlr.press/v108/morvan20a.html )

.. GENERATED FROM PYTHON SOURCE LINES 334-345

Predictive pipelines
-----------------------------

Now evaluate predictive pipelines:

- IterativeImputer and Ridge
- Mean imputation and Ridge
- IterativeImputer and HistGradientBoostingRegressor
- Mean imputation and HistGradientBoostingRegressor
- HistGradientBoostingRegressor directly on the data with missing
  values

.. GENERATED FROM PYTHON SOURCE LINES 349-350

What is the most important aspect of the pipeline?

.. GENERATED FROM PYTHON SOURCE LINES 353-358

Using a predictor for the fully-observed case
==============================================

Let us go back to the "easy" case of the missing completely at random
settings with plenty of data

.. GENERATED FROM PYTHON SOURCE LINES 358-362

.. code-block:: default

    n_samples = 20000

    X, y = generate_mcar(n_samples, missing_rate=.5)








.. GENERATED FROM PYTHON SOURCE LINES 363-365

Suppose we are able to train a predictive model that works on
fully-observed data:

.. GENERATED FROM PYTHON SOURCE LINES 365-370

.. code-block:: default


    X_full, y_full = generate_without_missing_values(n_samples)
    full_data_predictor = HistGradientBoostingRegressor()
    full_data_predictor.fit(X_full, y_full)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    HistGradientBoostingRegressor()



.. GENERATED FROM PYTHON SOURCE LINES 371-373

How can we know whether this model has captured well the link between
X_full and y_full?

.. GENERATED FROM PYTHON SOURCE LINES 375-378

Now turn to data with missing values. Given that the data is MAR
(missing at random), use imputation to build a completed data
that looks like the full-observed data

.. GENERATED FROM PYTHON SOURCE LINES 381-383

Apply the full data predictor on the imputed data and measure its
prediction score

.. GENERATED FROM PYTHON SOURCE LINES 383-385

.. code-block:: default

    from sklearn.metrics import r2_score








.. GENERATED FROM PYTHON SOURCE LINES 386-392

This prediction is less good than on the full data, but this is
expected, as missing values lead to a loss of information

Compare it to a model trained to predict on data with missing values
(Careful: can you use X and y to train you model? Keep in mind that you
can generate new data with `generate_mcar`)

.. GENERATED FROM PYTHON SOURCE LINES 394-396

How does a model valid on the full data applied to imputed data compare
to a model trained for missing values?

.. GENERATED FROM PYTHON SOURCE LINES 398-402

---------------------------------------

**Exercice:** now modify a bit the example above to consider the
situation where y is a non-linear function of X (a square)

.. GENERATED FROM PYTHON SOURCE LINES 402-406

.. code-block:: default


    X, y = generate_mcar(n_samples, missing_rate=.5)
    y = y ** 2








.. GENERATED FROM PYTHON SOURCE LINES 407-408

**Be careful to propagate the square every where that it is needed.

.. GENERATED FROM PYTHON SOURCE LINES 411-418

What do you observe? How does the performance of imputation + full-data
model compare to a model trained on data with missing values? Can you
explain this?

|

________


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  5.645 seconds)


.. _sphx_glr_download_gen_notes_01_missing_values.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/dirty-data-science/python/gh-pages?filepath=notes/gen_notes/01_missing_values.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 01_missing_values.py <01_missing_values.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 01_missing_values.ipynb <01_missing_values.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
