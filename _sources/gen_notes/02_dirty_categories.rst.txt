
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gen_notes/02_dirty_categories.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_gen_notes_02_dirty_categories.py>`
        to download the full example code. or to run this example in your browser via JupyterLite or Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gen_notes_02_dirty_categories.py:


========================================================
Dirty categories: learning with non normalized strings
========================================================

Including strings that represent categories often calls for much data
preparation. In particular categories may appear with many morphological
variants, when they have been manually input, or assembled from diverse
sources.

Including such a column in a learning pipeline as a standard categorical
colum leads to categories with very high cardinalities and can lose
information on which categories are similar.

Here we look at a dataset on wages [#]_ where the column *Employee
Position Title* contains dirty categories.

.. [#] https://catalog.data.gov/dataset/employee-salaries-2016

We investigate encodings to include such compare different categorical
encodings for the dirty column to predict the *Current Annual Salary*,
using gradient boosted trees. For this purpose, we use the skrub
library ( https://skrub-data.org ).

.. GENERATED FROM PYTHON SOURCE LINES 28-53

.. |SV| replace::
    :class:`~skrub.TableVectorizer`

.. |tabular_learner| replace::
    :func:`~skrub.tabular_learner`

.. |OneHotEncoder| replace::
    :class:`~sklearn.preprocessing.OneHotEncoder`

.. |RandomForestRegressor| replace::
    :class:`~sklearn.ensemble.RandomForestRegressor`

.. |SE| replace:: :class:`~skrub.SimilarityEncoder`

.. |permutation importances| replace::
    :func:`~sklearn.inspection.permutation_importance`


The data
========

Data Importing and preprocessing
--------------------------------

We first download the dataset:

.. GENERATED FROM PYTHON SOURCE LINES 54-58

.. code-block:: Python

    from skrub.datasets import fetch_employee_salaries
    employee_salaries = fetch_employee_salaries()
    print(employee_salaries.description)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Annual salary information including gross pay and overtime pay for all active, permanent employees of Montgomery County, MD paid in calendar year 2016. This information will be published annually each year.




.. GENERATED FROM PYTHON SOURCE LINES 59-60

Then we load it:

.. GENERATED FROM PYTHON SOURCE LINES 60-64

.. code-block:: Python

    import pandas as pd
    df = employee_salaries.X.copy()
    df






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>gender</th>
          <th>department</th>
          <th>department_name</th>
          <th>division</th>
          <th>assignment_category</th>
          <th>employee_position_title</th>
          <th>date_first_hired</th>
          <th>year_first_hired</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>F</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>MSB Information Mgmt and Tech Division Records...</td>
          <td>Fulltime-Regular</td>
          <td>Office Services Coordinator</td>
          <td>09/22/1986</td>
          <td>1986</td>
        </tr>
        <tr>
          <th>1</th>
          <td>M</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>ISB Major Crimes Division Fugitive Section</td>
          <td>Fulltime-Regular</td>
          <td>Master Police Officer</td>
          <td>09/12/1988</td>
          <td>1988</td>
        </tr>
        <tr>
          <th>2</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Adult Protective and Case Management Services</td>
          <td>Fulltime-Regular</td>
          <td>Social Worker IV</td>
          <td>11/19/1989</td>
          <td>1989</td>
        </tr>
        <tr>
          <th>3</th>
          <td>M</td>
          <td>COR</td>
          <td>Correction and Rehabilitation</td>
          <td>PRRS Facility and Security</td>
          <td>Fulltime-Regular</td>
          <td>Resident Supervisor II</td>
          <td>05/05/2014</td>
          <td>2014</td>
        </tr>
        <tr>
          <th>4</th>
          <td>M</td>
          <td>HCA</td>
          <td>Department of Housing and Community Affairs</td>
          <td>Affordable Housing Programs</td>
          <td>Fulltime-Regular</td>
          <td>Planning Specialist III</td>
          <td>03/05/2007</td>
          <td>2007</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>9223</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>School Based Health Centers</td>
          <td>Fulltime-Regular</td>
          <td>Community Health Nurse II</td>
          <td>11/03/2015</td>
          <td>2015</td>
        </tr>
        <tr>
          <th>9224</th>
          <td>F</td>
          <td>FRS</td>
          <td>Fire and Rescue Services</td>
          <td>Human Resources Division</td>
          <td>Fulltime-Regular</td>
          <td>Fire/Rescue Division Chief</td>
          <td>11/28/1988</td>
          <td>1988</td>
        </tr>
        <tr>
          <th>9225</th>
          <td>M</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Child and Adolescent Mental Health Clinic Serv...</td>
          <td>Parttime-Regular</td>
          <td>Medical Doctor IV - Psychiatrist</td>
          <td>04/30/2001</td>
          <td>2001</td>
        </tr>
        <tr>
          <th>9226</th>
          <td>M</td>
          <td>CCL</td>
          <td>County Council</td>
          <td>Council Central Staff</td>
          <td>Fulltime-Regular</td>
          <td>Manager II</td>
          <td>09/05/2006</td>
          <td>2006</td>
        </tr>
        <tr>
          <th>9227</th>
          <td>M</td>
          <td>DLC</td>
          <td>Department of Liquor Control</td>
          <td>Licensure, Regulation and Education</td>
          <td>Fulltime-Regular</td>
          <td>Alcohol/Tobacco Enforcement Specialist II</td>
          <td>01/30/2012</td>
          <td>2012</td>
        </tr>
      </tbody>
    </table>
    <p>9228 rows Ã— 8 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 65-66

Recover the target

.. GENERATED FROM PYTHON SOURCE LINES 66-69

.. code-block:: Python


    y = employee_salaries.y








.. GENERATED FROM PYTHON SOURCE LINES 70-75

A simple default as a learner
===============================

The function |tabular_learner| is a simple way of creating a default
learner for tabular_learner data:

.. GENERATED FROM PYTHON SOURCE LINES 76-79

.. code-block:: Python

    from skrub import tabular_learner
    model = tabular_learner("regressor")








.. GENERATED FROM PYTHON SOURCE LINES 80-82

We can quickly compute its cross-validation score using the
corresponding scikit-learn utility

.. GENERATED FROM PYTHON SOURCE LINES 82-88

.. code-block:: Python

    from sklearn.model_selection import cross_val_score
    import numpy as np

    results = cross_val_score(model, df, y)
    np.mean(results)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    np.float64(0.9108276644460975)



.. GENERATED FROM PYTHON SOURCE LINES 89-93

Understanding the pipeline
=======================================

Let's start again from the raw data:

.. GENERATED FROM PYTHON SOURCE LINES 93-97

.. code-block:: Python

    X = employee_salaries.X.copy()
    y = employee_salaries.y









.. GENERATED FROM PYTHON SOURCE LINES 98-99

We have a complex and heterogeneous dataframe:

.. GENERATED FROM PYTHON SOURCE LINES 99-101

.. code-block:: Python

    X






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>gender</th>
          <th>department</th>
          <th>department_name</th>
          <th>division</th>
          <th>assignment_category</th>
          <th>employee_position_title</th>
          <th>date_first_hired</th>
          <th>year_first_hired</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>F</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>MSB Information Mgmt and Tech Division Records...</td>
          <td>Fulltime-Regular</td>
          <td>Office Services Coordinator</td>
          <td>09/22/1986</td>
          <td>1986</td>
        </tr>
        <tr>
          <th>1</th>
          <td>M</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>ISB Major Crimes Division Fugitive Section</td>
          <td>Fulltime-Regular</td>
          <td>Master Police Officer</td>
          <td>09/12/1988</td>
          <td>1988</td>
        </tr>
        <tr>
          <th>2</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Adult Protective and Case Management Services</td>
          <td>Fulltime-Regular</td>
          <td>Social Worker IV</td>
          <td>11/19/1989</td>
          <td>1989</td>
        </tr>
        <tr>
          <th>3</th>
          <td>M</td>
          <td>COR</td>
          <td>Correction and Rehabilitation</td>
          <td>PRRS Facility and Security</td>
          <td>Fulltime-Regular</td>
          <td>Resident Supervisor II</td>
          <td>05/05/2014</td>
          <td>2014</td>
        </tr>
        <tr>
          <th>4</th>
          <td>M</td>
          <td>HCA</td>
          <td>Department of Housing and Community Affairs</td>
          <td>Affordable Housing Programs</td>
          <td>Fulltime-Regular</td>
          <td>Planning Specialist III</td>
          <td>03/05/2007</td>
          <td>2007</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>9223</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>School Based Health Centers</td>
          <td>Fulltime-Regular</td>
          <td>Community Health Nurse II</td>
          <td>11/03/2015</td>
          <td>2015</td>
        </tr>
        <tr>
          <th>9224</th>
          <td>F</td>
          <td>FRS</td>
          <td>Fire and Rescue Services</td>
          <td>Human Resources Division</td>
          <td>Fulltime-Regular</td>
          <td>Fire/Rescue Division Chief</td>
          <td>11/28/1988</td>
          <td>1988</td>
        </tr>
        <tr>
          <th>9225</th>
          <td>M</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Child and Adolescent Mental Health Clinic Serv...</td>
          <td>Parttime-Regular</td>
          <td>Medical Doctor IV - Psychiatrist</td>
          <td>04/30/2001</td>
          <td>2001</td>
        </tr>
        <tr>
          <th>9226</th>
          <td>M</td>
          <td>CCL</td>
          <td>County Council</td>
          <td>Council Central Staff</td>
          <td>Fulltime-Regular</td>
          <td>Manager II</td>
          <td>09/05/2006</td>
          <td>2006</td>
        </tr>
        <tr>
          <th>9227</th>
          <td>M</td>
          <td>DLC</td>
          <td>Department of Liquor Control</td>
          <td>Licensure, Regulation and Education</td>
          <td>Fulltime-Regular</td>
          <td>Alcohol/Tobacco Enforcement Specialist II</td>
          <td>01/30/2012</td>
          <td>2012</td>
        </tr>
      </tbody>
    </table>
    <p>9228 rows Ã— 8 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 102-104

The |SV| can to turn this dataframe into a form suited for
machine learning.

.. GENERATED FROM PYTHON SOURCE LINES 106-112

Using the TableVectorizer in a supervised-learning pipeline
------------------------------------------------------------

Assembling the |SV| in a pipeline with a powerful learner,
such as gradient boosted trees, gives **a machine-learning method that
can be readily applied to the dataframe**.

.. GENERATED FROM PYTHON SOURCE LINES 112-114

.. code-block:: Python

    from skrub import TableVectorizer








.. GENERATED FROM PYTHON SOURCE LINES 115-117

We will use it with a HistGradientBoostingRegressor, which is a good
predictor for data with heterogeneous columns

.. GENERATED FROM PYTHON SOURCE LINES 117-119

.. code-block:: Python

    from sklearn.ensemble import HistGradientBoostingRegressor








.. GENERATED FROM PYTHON SOURCE LINES 120-121

We then create a pipeline chaining our encoders to a learner

.. GENERATED FROM PYTHON SOURCE LINES 121-128

.. code-block:: Python

    from sklearn.pipeline import make_pipeline

    pipeline = make_pipeline(
        TableVectorizer(),
        HistGradientBoostingRegressor()
    )








.. GENERATED FROM PYTHON SOURCE LINES 129-130

Let's perform a cross-validation to see how well this model predicts

.. GENERATED FROM PYTHON SOURCE LINES 130-139

.. code-block:: Python


    from sklearn.model_selection import cross_val_score
    scores = cross_val_score(pipeline, X, y, scoring='r2')

    import numpy as np
    print(f'{scores=}')
    print(f'mean={np.mean(scores)}')
    print(f'std={np.std(scores)}')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    scores=array([0.91722464, 0.89540173, 0.93245198, 0.92736712, 0.93651227])
    mean=0.9217915470850576
    std=0.014688161834584452




.. GENERATED FROM PYTHON SOURCE LINES 140-143

The prediction perform here is pretty much as good as above
but the code here is much simpler as it does not involve specifying
columns manually.

.. GENERATED FROM PYTHON SOURCE LINES 145-150

Analyzing the features created
-------------------------------

Let us perform the same workflow, but without the `Pipeline`, so we can
analyze its mechanisms along the way.

.. GENERATED FROM PYTHON SOURCE LINES 150-152

.. code-block:: Python

    tab_vec = TableVectorizer()








.. GENERATED FROM PYTHON SOURCE LINES 153-154

We split the data between train and test, and transform them:

.. GENERATED FROM PYTHON SOURCE LINES 154-162

.. code-block:: Python

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42
    )

    X_train_enc = tab_vec.fit_transform(X_train, y_train)
    X_test_enc = tab_vec.transform(X_test)








.. GENERATED FROM PYTHON SOURCE LINES 163-164

The encoded data, X_train_enc and X_test_enc are numerical arrays:

.. GENERATED FROM PYTHON SOURCE LINES 164-166

.. code-block:: Python

    X_train_enc






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>gender_F</th>
          <th>gender_M</th>
          <th>gender_nan</th>
          <th>department_BOA</th>
          <th>department_BOE</th>
          <th>department_CAT</th>
          <th>department_CCL</th>
          <th>department_CEC</th>
          <th>department_CEX</th>
          <th>department_COR</th>
          <th>department_CUS</th>
          <th>department_DEP</th>
          <th>department_DGS</th>
          <th>department_DHS</th>
          <th>department_DLC</th>
          <th>department_DOT</th>
          <th>department_DPS</th>
          <th>department_DTS</th>
          <th>department_ECM</th>
          <th>department_FIN</th>
          <th>department_FRS</th>
          <th>department_HCA</th>
          <th>department_HHS</th>
          <th>department_HRC</th>
          <th>department_IGR</th>
          <th>department_LIB</th>
          <th>department_MPB</th>
          <th>department_NDA</th>
          <th>department_OAG</th>
          <th>department_OCP</th>
          <th>department_OHR</th>
          <th>department_OIG</th>
          <th>department_OLO</th>
          <th>department_OMB</th>
          <th>department_PIO</th>
          <th>department_POL</th>
          <th>department_PRO</th>
          <th>department_REC</th>
          <th>department_SHF</th>
          <th>department_ZAH</th>
          <th>...</th>
          <th>division: investigative, investigations, criminal</th>
          <th>division: building, design, construction</th>
          <th>division: nicholson, transit, taxicab</th>
          <th>division: accounts, development, landlord</th>
          <th>assignment_category_Parttime-Regular</th>
          <th>employee_position_title: officer, office, traffic</th>
          <th>employee_position_title: operator, equipment, bus</th>
          <th>employee_position_title: lieutenant, captain, chief</th>
          <th>employee_position_title: specialist, special, procurement</th>
          <th>employee_position_title: firefighter, rescuer, master</th>
          <th>employee_position_title: community, security, health</th>
          <th>employee_position_title: information, technologist, technology</th>
          <th>employee_position_title: manager, budget, investigator</th>
          <th>employee_position_title: school, room, pool</th>
          <th>employee_position_title: enforcement, permitting, inspector</th>
          <th>employee_position_title: liquor, clerk, store</th>
          <th>employee_position_title: coordinator, coordinating, transit</th>
          <th>employee_position_title: technician, mechanic, supply</th>
          <th>employee_position_title: legislative, principal, executive</th>
          <th>employee_position_title: associate, library, appellate</th>
          <th>employee_position_title: correctional, correction, environmental</th>
          <th>employee_position_title: recreation, renovation, resource</th>
          <th>employee_position_title: crossing, purchasing, background</th>
          <th>employee_position_title: accountant, assistant, attorney</th>
          <th>employee_position_title: administrative, administration, administrator</th>
          <th>employee_position_title: corporal, pfc, private</th>
          <th>employee_position_title: sergeant, police, cadet</th>
          <th>employee_position_title: craftsworker, public, urban</th>
          <th>employee_position_title: warehouse, welfare, caseworker</th>
          <th>employee_position_title: psychiatric, employee, attendant</th>
          <th>employee_position_title: therapist, sheriff, deputy</th>
          <th>employee_position_title: communications, telecommunications, safety</th>
          <th>employee_position_title: income, assistance, client</th>
          <th>employee_position_title: services, service, president</th>
          <th>employee_position_title: program, programs, projects</th>
          <th>date_first_hired_year</th>
          <th>date_first_hired_month</th>
          <th>date_first_hired_day</th>
          <th>date_first_hired_total_seconds</th>
          <th>year_first_hired</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>4405</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.082526</td>
          <td>0.086296</td>
          <td>0.065845</td>
          <td>1.158715</td>
          <td>0.0</td>
          <td>0.055234</td>
          <td>3.898660</td>
          <td>0.102113</td>
          <td>18.377623</td>
          <td>0.095721</td>
          <td>0.053326</td>
          <td>0.237525</td>
          <td>0.064906</td>
          <td>0.053707</td>
          <td>4.643124</td>
          <td>0.054317</td>
          <td>0.097979</td>
          <td>0.061125</td>
          <td>0.108701</td>
          <td>0.088707</td>
          <td>0.060723</td>
          <td>0.098018</td>
          <td>2.553727</td>
          <td>0.306366</td>
          <td>0.317221</td>
          <td>0.054437</td>
          <td>0.056922</td>
          <td>0.055537</td>
          <td>0.066883</td>
          <td>17.134558</td>
          <td>0.082473</td>
          <td>0.069323</td>
          <td>0.118592</td>
          <td>0.479933</td>
          <td>0.052522</td>
          <td>2007.0</td>
          <td>8.0</td>
          <td>6.0</td>
          <td>1.186358e+09</td>
          <td>2007.0</td>
        </tr>
        <tr>
          <th>5694</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.052668</td>
          <td>0.051270</td>
          <td>0.055674</td>
          <td>0.050959</td>
          <td>1.0</td>
          <td>0.054969</td>
          <td>0.050001</td>
          <td>0.050910</td>
          <td>0.053845</td>
          <td>0.052150</td>
          <td>34.419083</td>
          <td>0.052498</td>
          <td>0.057989</td>
          <td>0.066735</td>
          <td>0.053037</td>
          <td>0.052251</td>
          <td>0.052029</td>
          <td>0.054304</td>
          <td>0.051995</td>
          <td>0.052480</td>
          <td>0.053473</td>
          <td>0.051355</td>
          <td>0.050357</td>
          <td>0.052719</td>
          <td>0.054325</td>
          <td>0.054313</td>
          <td>0.053712</td>
          <td>0.052253</td>
          <td>0.054209</td>
          <td>0.051336</td>
          <td>0.058101</td>
          <td>0.061700</td>
          <td>0.054547</td>
          <td>0.071293</td>
          <td>0.052029</td>
          <td>2005.0</td>
          <td>8.0</td>
          <td>8.0</td>
          <td>1.123459e+09</td>
          <td>2005.0</td>
        </tr>
        <tr>
          <th>1516</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.077653</td>
          <td>44.600155</td>
          <td>0.061732</td>
          <td>0.063686</td>
          <td>0.0</td>
          <td>0.052739</td>
          <td>0.058493</td>
          <td>0.097883</td>
          <td>0.051748</td>
          <td>0.057744</td>
          <td>0.104592</td>
          <td>16.383930</td>
          <td>0.054830</td>
          <td>0.062572</td>
          <td>2.441779</td>
          <td>0.058371</td>
          <td>0.058777</td>
          <td>0.076688</td>
          <td>0.122113</td>
          <td>0.208777</td>
          <td>0.297332</td>
          <td>0.060926</td>
          <td>0.105378</td>
          <td>0.078577</td>
          <td>0.052933</td>
          <td>0.112364</td>
          <td>0.053717</td>
          <td>0.058363</td>
          <td>0.060913</td>
          <td>1.399023</td>
          <td>0.050618</td>
          <td>0.052706</td>
          <td>0.060446</td>
          <td>0.110604</td>
          <td>0.055065</td>
          <td>2009.0</td>
          <td>4.0</td>
          <td>27.0</td>
          <td>1.240790e+09</td>
          <td>2009.0</td>
        </tr>
        <tr>
          <th>8960</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.054070</td>
          <td>0.053580</td>
          <td>0.052682</td>
          <td>0.055198</td>
          <td>0.0</td>
          <td>0.070402</td>
          <td>0.055064</td>
          <td>11.076848</td>
          <td>0.059618</td>
          <td>0.075034</td>
          <td>0.081233</td>
          <td>0.064251</td>
          <td>1.512149</td>
          <td>0.059238</td>
          <td>0.081923</td>
          <td>0.064362</td>
          <td>0.067165</td>
          <td>0.059916</td>
          <td>0.057092</td>
          <td>0.055138</td>
          <td>45.472214</td>
          <td>0.062002</td>
          <td>0.050705</td>
          <td>0.078197</td>
          <td>0.055524</td>
          <td>0.091452</td>
          <td>0.085584</td>
          <td>0.063715</td>
          <td>0.067490</td>
          <td>0.066738</td>
          <td>0.108211</td>
          <td>0.094479</td>
          <td>0.063388</td>
          <td>0.079625</td>
          <td>0.121242</td>
          <td>1997.0</td>
          <td>2.0</td>
          <td>3.0</td>
          <td>8.549280e+08</td>
          <td>1997.0</td>
        </tr>
        <tr>
          <th>6108</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.054742</td>
          <td>0.055978</td>
          <td>0.053776</td>
          <td>0.056543</td>
          <td>0.0</td>
          <td>23.824232</td>
          <td>0.053066</td>
          <td>0.052212</td>
          <td>0.054112</td>
          <td>0.061967</td>
          <td>0.053349</td>
          <td>0.052471</td>
          <td>0.089135</td>
          <td>0.051806</td>
          <td>0.056017</td>
          <td>0.055547</td>
          <td>0.062418</td>
          <td>0.054329</td>
          <td>0.050839</td>
          <td>0.052227</td>
          <td>0.051996</td>
          <td>0.050648</td>
          <td>0.050141</td>
          <td>0.051632</td>
          <td>0.054363</td>
          <td>0.074501</td>
          <td>0.086072</td>
          <td>0.061946</td>
          <td>0.060725</td>
          <td>0.051805</td>
          <td>0.057434</td>
          <td>0.053072</td>
          <td>0.054386</td>
          <td>0.059162</td>
          <td>0.058389</td>
          <td>2006.0</td>
          <td>1.0</td>
          <td>17.0</td>
          <td>1.137456e+09</td>
          <td>2006.0</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>5734</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.058358</td>
          <td>0.052813</td>
          <td>0.052566</td>
          <td>0.050397</td>
          <td>0.0</td>
          <td>0.056383</td>
          <td>0.051916</td>
          <td>0.073191</td>
          <td>0.054357</td>
          <td>35.917694</td>
          <td>0.050080</td>
          <td>0.054274</td>
          <td>0.064327</td>
          <td>0.051384</td>
          <td>0.050983</td>
          <td>0.057324</td>
          <td>0.051866</td>
          <td>0.051953</td>
          <td>0.052464</td>
          <td>0.053239</td>
          <td>0.052806</td>
          <td>0.055544</td>
          <td>0.050341</td>
          <td>0.059061</td>
          <td>0.055767</td>
          <td>0.052774</td>
          <td>0.052107</td>
          <td>0.052410</td>
          <td>0.054254</td>
          <td>0.051830</td>
          <td>0.054092</td>
          <td>0.050940</td>
          <td>0.055622</td>
          <td>0.053007</td>
          <td>0.058012</td>
          <td>2005.0</td>
          <td>5.0</td>
          <td>16.0</td>
          <td>1.116202e+09</td>
          <td>2005.0</td>
        </tr>
        <tr>
          <th>5191</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.055469</td>
          <td>0.080793</td>
          <td>0.052181</td>
          <td>0.062110</td>
          <td>0.0</td>
          <td>0.067366</td>
          <td>0.052561</td>
          <td>0.072747</td>
          <td>0.064287</td>
          <td>0.067213</td>
          <td>0.087528</td>
          <td>0.075529</td>
          <td>0.086105</td>
          <td>0.054536</td>
          <td>0.162606</td>
          <td>0.059725</td>
          <td>0.062825</td>
          <td>0.060091</td>
          <td>5.711314</td>
          <td>0.055966</td>
          <td>0.214050</td>
          <td>26.667959</td>
          <td>0.055666</td>
          <td>0.105595</td>
          <td>0.523478</td>
          <td>0.134657</td>
          <td>0.075274</td>
          <td>0.061453</td>
          <td>0.082092</td>
          <td>0.061432</td>
          <td>0.075137</td>
          <td>0.074209</td>
          <td>0.071760</td>
          <td>8.487636</td>
          <td>0.069202</td>
          <td>2001.0</td>
          <td>8.0</td>
          <td>6.0</td>
          <td>9.970560e+08</td>
          <td>2001.0</td>
        </tr>
        <tr>
          <th>5390</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.076380</td>
          <td>50.095226</td>
          <td>0.147106</td>
          <td>0.058520</td>
          <td>0.0</td>
          <td>0.099258</td>
          <td>0.123462</td>
          <td>0.051216</td>
          <td>0.080651</td>
          <td>0.085073</td>
          <td>0.066852</td>
          <td>0.080458</td>
          <td>0.483042</td>
          <td>0.100922</td>
          <td>6.959088</td>
          <td>0.058760</td>
          <td>0.060779</td>
          <td>20.596811</td>
          <td>0.057509</td>
          <td>0.059185</td>
          <td>0.064303</td>
          <td>0.059648</td>
          <td>0.050470</td>
          <td>0.060013</td>
          <td>0.087908</td>
          <td>0.068117</td>
          <td>0.066632</td>
          <td>0.066488</td>
          <td>0.081976</td>
          <td>0.060997</td>
          <td>0.086280</td>
          <td>0.061724</td>
          <td>0.059918</td>
          <td>0.098170</td>
          <td>0.064292</td>
          <td>1990.0</td>
          <td>5.0</td>
          <td>31.0</td>
          <td>6.441120e+08</td>
          <td>1990.0</td>
        </tr>
        <tr>
          <th>860</th>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.087792</td>
          <td>0.089230</td>
          <td>0.063692</td>
          <td>0.100628</td>
          <td>0.0</td>
          <td>0.054715</td>
          <td>0.116939</td>
          <td>0.055366</td>
          <td>0.059659</td>
          <td>0.062790</td>
          <td>0.088616</td>
          <td>0.055021</td>
          <td>0.054197</td>
          <td>0.071810</td>
          <td>0.063383</td>
          <td>0.059963</td>
          <td>0.054079</td>
          <td>0.050704</td>
          <td>0.062869</td>
          <td>0.058405</td>
          <td>0.058087</td>
          <td>0.062217</td>
          <td>0.053713</td>
          <td>0.051396</td>
          <td>0.066472</td>
          <td>0.053415</td>
          <td>0.057449</td>
          <td>0.062018</td>
          <td>50.722256</td>
          <td>0.057398</td>
          <td>0.069484</td>
          <td>0.053946</td>
          <td>0.055402</td>
          <td>0.055785</td>
          <td>0.052447</td>
          <td>2012.0</td>
          <td>11.0</td>
          <td>5.0</td>
          <td>1.352074e+09</td>
          <td>2012.0</td>
        </tr>
        <tr>
          <th>7270</th>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>1.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>0.0</td>
          <td>...</td>
          <td>0.052328</td>
          <td>0.051883</td>
          <td>0.052241</td>
          <td>0.051691</td>
          <td>0.0</td>
          <td>0.069777</td>
          <td>0.053407</td>
          <td>0.050002</td>
          <td>0.077289</td>
          <td>0.061996</td>
          <td>0.055056</td>
          <td>0.058398</td>
          <td>0.098837</td>
          <td>0.057505</td>
          <td>0.057744</td>
          <td>0.058682</td>
          <td>0.052411</td>
          <td>0.059579</td>
          <td>0.053016</td>
          <td>0.060605</td>
          <td>0.063166</td>
          <td>0.057939</td>
          <td>0.050184</td>
          <td>0.052616</td>
          <td>0.062718</td>
          <td>0.067996</td>
          <td>0.051618</td>
          <td>0.068640</td>
          <td>20.760992</td>
          <td>0.053843</td>
          <td>0.056778</td>
          <td>0.054392</td>
          <td>0.056945</td>
          <td>0.055015</td>
          <td>0.062854</td>
          <td>2014.0</td>
          <td>6.0</td>
          <td>16.0</td>
          <td>1.402877e+09</td>
          <td>2014.0</td>
        </tr>
      </tbody>
    </table>
    <p>7843 rows Ã— 143 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 167-168

They have more columns than the original dataframe, but not much more:

.. GENERATED FROM PYTHON SOURCE LINES 168-170

.. code-block:: Python

    X_train_enc.shape





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    (7843, 143)



.. GENERATED FROM PYTHON SOURCE LINES 171-176

Inspecting the features created
.................................

The |SV| assigns a transformer for each column. We can inspect this
choice:

.. GENERATED FROM PYTHON SOURCE LINES 176-178

.. code-block:: Python

    tab_vec.transformers_





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    {'year_first_hired': PassThrough(), 'date_first_hired': DatetimeEncoder(), 'gender': OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
                  sparse_output=False), 'department': OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
                  sparse_output=False), 'department_name': OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
                  sparse_output=False), 'assignment_category': OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
                  sparse_output=False), 'division': GapEncoder(n_components=30), 'employee_position_title': GapEncoder(n_components=30)}



.. GENERATED FROM PYTHON SOURCE LINES 179-191

This is what is being passed to transform the different columns under the hood.
We can notice it classified the columns "gender" and "assignment_category"
as low cardinality string variables.
A |OneHotEncoder| will be applied to these columns.

The vectorizer actually makes the difference between string variables
(data type ``object`` and ``string``) and categorical variables
(data type ``category``).

Next, we can have a look at the encoded feature names.

Before encoding:

.. GENERATED FROM PYTHON SOURCE LINES 191-193

.. code-block:: Python

    X.columns.to_list()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title', 'date_first_hired', 'year_first_hired']



.. GENERATED FROM PYTHON SOURCE LINES 194-195

After encoding (we only plot the first 8 feature names):

.. GENERATED FROM PYTHON SOURCE LINES 195-198

.. code-block:: Python

    feature_names = tab_vec.get_feature_names_out()
    feature_names[:8]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    array(['gender_F', 'gender_M', 'gender_nan', 'department_BOA',
           'department_BOE', 'department_CAT', 'department_CCL',
           'department_CEC'], dtype='<U70')



.. GENERATED FROM PYTHON SOURCE LINES 199-205

As we can see, it created a new column for each unique value.
This is because we used |SE| on the column "division",
which was classified as a high cardinality string variable.
(default values, see |SV|'s docstring).

In total, we have reasonnable number of encoded columns.

.. GENERATED FROM PYTHON SOURCE LINES 205-208

.. code-block:: Python

    len(feature_names)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    143



.. GENERATED FROM PYTHON SOURCE LINES 209-221

Feature importance in the statistical model
---------------------------------------------

In this section, we will train a regressor, and plot the feature importances

.. topic:: Note:

   To minimize compute time, use the feature importances computed by the
   |RandomForestRegressor|, but you should prefer |permutation importances|
   instead (which are less subject to biases)

First, let's train the |RandomForestRegressor|,

.. GENERATED FROM PYTHON SOURCE LINES 221-227

.. code-block:: Python


    from sklearn.ensemble import RandomForestRegressor
    regressor = RandomForestRegressor()
    regressor.fit(X_train_enc, y_train)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-1 {
      /* Definition of color scheme common for light and dark mode */
      --sklearn-color-text: black;
      --sklearn-color-line: gray;
      /* Definition of color scheme for unfitted estimators */
      --sklearn-color-unfitted-level-0: #fff5e6;
      --sklearn-color-unfitted-level-1: #f6e4d2;
      --sklearn-color-unfitted-level-2: #ffe0b3;
      --sklearn-color-unfitted-level-3: chocolate;
      /* Definition of color scheme for fitted estimators */
      --sklearn-color-fitted-level-0: #f0f8ff;
      --sklearn-color-fitted-level-1: #d4ebff;
      --sklearn-color-fitted-level-2: #b3dbfd;
      --sklearn-color-fitted-level-3: cornflowerblue;

      /* Specific color for light theme */
      --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
      --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
      --sklearn-color-icon: #696969;

      @media (prefers-color-scheme: dark) {
        /* Redefinition of color scheme for dark theme */
        --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
        --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
        --sklearn-color-icon: #878787;
      }
    }

    #sk-container-id-1 {
      color: var(--sklearn-color-text);
    }

    #sk-container-id-1 pre {
      padding: 0;
    }

    #sk-container-id-1 input.sk-hidden--visually {
      border: 0;
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      height: 1px;
      margin: -1px;
      overflow: hidden;
      padding: 0;
      position: absolute;
      width: 1px;
    }

    #sk-container-id-1 div.sk-dashed-wrapped {
      border: 1px dashed var(--sklearn-color-line);
      margin: 0 0.4em 0.5em 0.4em;
      box-sizing: border-box;
      padding-bottom: 0.4em;
      background-color: var(--sklearn-color-background);
    }

    #sk-container-id-1 div.sk-container {
      /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
         but bootstrap.min.css set `[hidden] { display: none !important; }`
         so we also need the `!important` here to be able to override the
         default hidden behavior on the sphinx rendered scikit-learn.org.
         See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
      display: inline-block !important;
      position: relative;
    }

    #sk-container-id-1 div.sk-text-repr-fallback {
      display: none;
    }

    div.sk-parallel-item,
    div.sk-serial,
    div.sk-item {
      /* draw centered vertical line to link estimators */
      background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
      background-size: 2px 100%;
      background-repeat: no-repeat;
      background-position: center center;
    }

    /* Parallel-specific style estimator block */

    #sk-container-id-1 div.sk-parallel-item::after {
      content: "";
      width: 100%;
      border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
      flex-grow: 1;
    }

    #sk-container-id-1 div.sk-parallel {
      display: flex;
      align-items: stretch;
      justify-content: center;
      background-color: var(--sklearn-color-background);
      position: relative;
    }

    #sk-container-id-1 div.sk-parallel-item {
      display: flex;
      flex-direction: column;
    }

    #sk-container-id-1 div.sk-parallel-item:first-child::after {
      align-self: flex-end;
      width: 50%;
    }

    #sk-container-id-1 div.sk-parallel-item:last-child::after {
      align-self: flex-start;
      width: 50%;
    }

    #sk-container-id-1 div.sk-parallel-item:only-child::after {
      width: 0;
    }

    /* Serial-specific style estimator block */

    #sk-container-id-1 div.sk-serial {
      display: flex;
      flex-direction: column;
      align-items: center;
      background-color: var(--sklearn-color-background);
      padding-right: 1em;
      padding-left: 1em;
    }


    /* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
    clickable and can be expanded/collapsed.
    - Pipeline and ColumnTransformer use this feature and define the default style
    - Estimators will overwrite some part of the style using the `sk-estimator` class
    */

    /* Pipeline and ColumnTransformer style (default) */

    #sk-container-id-1 div.sk-toggleable {
      /* Default theme specific background. It is overwritten whether we have a
      specific estimator or a Pipeline/ColumnTransformer */
      background-color: var(--sklearn-color-background);
    }

    /* Toggleable label */
    #sk-container-id-1 label.sk-toggleable__label {
      cursor: pointer;
      display: block;
      width: 100%;
      margin-bottom: 0;
      padding: 0.5em;
      box-sizing: border-box;
      text-align: center;
    }

    #sk-container-id-1 label.sk-toggleable__label-arrow:before {
      /* Arrow on the left of the label */
      content: "â–¸";
      float: left;
      margin-right: 0.25em;
      color: var(--sklearn-color-icon);
    }

    #sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
      color: var(--sklearn-color-text);
    }

    /* Toggleable content - dropdown */

    #sk-container-id-1 div.sk-toggleable__content {
      max-height: 0;
      max-width: 0;
      overflow: hidden;
      text-align: left;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-1 div.sk-toggleable__content.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-1 div.sk-toggleable__content pre {
      margin: 0.2em;
      border-radius: 0.25em;
      color: var(--sklearn-color-text);
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-1 div.sk-toggleable__content.fitted pre {
      /* unfitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    #sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
      /* Expand drop-down */
      max-height: 200px;
      max-width: 100%;
      overflow: auto;
    }

    #sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
      content: "â–¾";
    }

    /* Pipeline/ColumnTransformer-specific style */

    #sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator-specific style */

    /* Colorize estimator box */
    #sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    #sk-container-id-1 div.sk-label label.sk-toggleable__label,
    #sk-container-id-1 div.sk-label label {
      /* The background is the default theme color */
      color: var(--sklearn-color-text-on-default-background);
    }

    /* On hover, darken the color of the background */
    #sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    /* Label box, darken color on hover, fitted */
    #sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
      color: var(--sklearn-color-text);
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Estimator label */

    #sk-container-id-1 div.sk-label label {
      font-family: monospace;
      font-weight: bold;
      display: inline-block;
      line-height: 1.2em;
    }

    #sk-container-id-1 div.sk-label-container {
      text-align: center;
    }

    /* Estimator-specific */
    #sk-container-id-1 div.sk-estimator {
      font-family: monospace;
      border: 1px dotted var(--sklearn-color-border-box);
      border-radius: 0.25em;
      box-sizing: border-box;
      margin-bottom: 0.5em;
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-0);
    }

    #sk-container-id-1 div.sk-estimator.fitted {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-0);
    }

    /* on hover */
    #sk-container-id-1 div.sk-estimator:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-2);
    }

    #sk-container-id-1 div.sk-estimator.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-2);
    }

    /* Specification for estimator info (e.g. "i" and "?") */

    /* Common style for "i" and "?" */

    .sk-estimator-doc-link,
    a:link.sk-estimator-doc-link,
    a:visited.sk-estimator-doc-link {
      float: right;
      font-size: smaller;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1em;
      height: 1em;
      width: 1em;
      text-decoration: none !important;
      margin-left: 1ex;
      /* unfitted */
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
      color: var(--sklearn-color-unfitted-level-1);
    }

    .sk-estimator-doc-link.fitted,
    a:link.sk-estimator-doc-link.fitted,
    a:visited.sk-estimator-doc-link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    div.sk-estimator:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover,
    div.sk-label-container:hover .sk-estimator-doc-link:hover,
    .sk-estimator-doc-link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover,
    div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
    .sk-estimator-doc-link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    /* Span, style for the box shown on hovering the info icon */
    .sk-estimator-doc-link span {
      display: none;
      z-index: 9999;
      position: relative;
      font-weight: normal;
      right: .2ex;
      padding: .5ex;
      margin: .5ex;
      width: min-content;
      min-width: 20ex;
      max-width: 50ex;
      color: var(--sklearn-color-text);
      box-shadow: 2pt 2pt 4pt #999;
      /* unfitted */
      background: var(--sklearn-color-unfitted-level-0);
      border: .5pt solid var(--sklearn-color-unfitted-level-3);
    }

    .sk-estimator-doc-link.fitted span {
      /* fitted */
      background: var(--sklearn-color-fitted-level-0);
      border: var(--sklearn-color-fitted-level-3);
    }

    .sk-estimator-doc-link:hover span {
      display: block;
    }

    /* "?"-specific style due to the `<a>` HTML tag */

    #sk-container-id-1 a.estimator_doc_link {
      float: right;
      font-size: 1rem;
      line-height: 1em;
      font-family: monospace;
      background-color: var(--sklearn-color-background);
      border-radius: 1rem;
      height: 1rem;
      width: 1rem;
      text-decoration: none;
      /* unfitted */
      color: var(--sklearn-color-unfitted-level-1);
      border: var(--sklearn-color-unfitted-level-1) 1pt solid;
    }

    #sk-container-id-1 a.estimator_doc_link.fitted {
      /* fitted */
      border: var(--sklearn-color-fitted-level-1) 1pt solid;
      color: var(--sklearn-color-fitted-level-1);
    }

    /* On hover */
    #sk-container-id-1 a.estimator_doc_link:hover {
      /* unfitted */
      background-color: var(--sklearn-color-unfitted-level-3);
      color: var(--sklearn-color-background);
      text-decoration: none;
    }

    #sk-container-id-1 a.estimator_doc_link.fitted:hover {
      /* fitted */
      background-color: var(--sklearn-color-fitted-level-3);
    }
    </style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;RandomForestRegressor<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html">?<span>Documentation for RandomForestRegressor</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>RandomForestRegressor()</pre></div> </div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 228-229

Retreiving the feature importances

.. GENERATED FROM PYTHON SOURCE LINES 229-239

.. code-block:: Python

    importances = regressor.feature_importances_
    std = np.std(
        [
            tree.feature_importances_
            for tree in regressor.estimators_
        ],
        axis=0
    )
    indices = np.argsort(importances)[::-1]








.. GENERATED FROM PYTHON SOURCE LINES 240-241

Plotting the results:

.. GENERATED FROM PYTHON SOURCE LINES 241-253

.. code-block:: Python


    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 9))
    plt.title("Feature importances")
    n = 20
    n_indices = indices[:n]
    labels = np.array(feature_names)[n_indices]
    plt.barh(range(n), importances[n_indices], color="b", yerr=std[n_indices])
    plt.yticks(range(n), labels, size=15)
    plt.tight_layout(pad=1)
    plt.show()




.. image-sg:: /gen_notes/images/sphx_glr_02_dirty_categories_001.png
   :alt: Feature importances
   :srcset: /gen_notes/images/sphx_glr_02_dirty_categories_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 254-257

We can deduce from this data that the three factors that define the
most the salary are: being hired for a long time, being a manager, and
having a permanent, full-time job :).

.. GENERATED FROM PYTHON SOURCE LINES 260-268

Exploring different machine-learning pipeline to encode the data
=================================================================

The learning pipeline
----------------------------

To build a learning pipeline, we need to assemble encoders for each
column, and apply a supervised learning model on top.

.. GENERATED FROM PYTHON SOURCE LINES 271-276

Encoding the table
........................

The TableVectorizer applies different transformations to the different
columns to turn them into numerical values suitable for learning

.. GENERATED FROM PYTHON SOURCE LINES 276-280

.. code-block:: Python


    from skrub import TableVectorizer
    encoder = TableVectorizer()








.. GENERATED FROM PYTHON SOURCE LINES 281-285

Pipelining an encoder with a learner
....................................

Here again we use a pipeline with HistGradientBoostingRegressor

.. GENERATED FROM PYTHON SOURCE LINES 285-288

.. code-block:: Python

    from sklearn.ensemble import HistGradientBoostingRegressor
    pipeline = make_pipeline(encoder, HistGradientBoostingRegressor())








.. GENERATED FROM PYTHON SOURCE LINES 289-290

The pipeline can be readily applied to the dataframe for prediction

.. GENERATED FROM PYTHON SOURCE LINES 290-301

.. code-block:: Python

    pipeline.fit(df, y)

    # The categorical encoders
    # ........................
    #
    # A encoder is needed to turn a categorical column into a numerical
    # representation
    from sklearn.preprocessing import OneHotEncoder

    one_hot = OneHotEncoder(handle_unknown='ignore', sparse_output=False)








.. GENERATED FROM PYTHON SOURCE LINES 302-309

Dirty-category encoding
-------------------------

The one-hot encoder is actually not well suited to the 'Employee
Position Title' column, as this columns contains 400 different entries.

We will now experiments with different encoders for dirty columns

.. GENERATED FROM PYTHON SOURCE LINES 309-325

.. code-block:: Python

    from skrub import SimilarityEncoder, MinHashEncoder,\
        GapEncoder
    #TargetEncoder, 

    similarity = SimilarityEncoder()
    #target = TargetEncoder(handle_unknown='ignore')
    minhash = MinHashEncoder(n_components=100)
    gap = GapEncoder(n_components=100)

    encoders = {
        'one-hot': one_hot,
        'similarity': similarity,
        #'target': target,
        'minhash': minhash,
        'gap': gap}








.. GENERATED FROM PYTHON SOURCE LINES 326-329

We now loop over the different encoding methods,
instantiate each time a new pipeline, fit it
and store the returned cross-validation score:

.. GENERATED FROM PYTHON SOURCE LINES 329-342

.. code-block:: Python


    all_scores = dict()

    for name, method in encoders.items():
        encoder = TableVectorizer(high_cardinality=method)

        pipeline = make_pipeline(encoder, HistGradientBoostingRegressor())
        scores = cross_val_score(pipeline, df, y)
        print('{} encoding'.format(name))
        print('r2 score:  mean: {:.3f}; std: {:.3f}\n'.format(
            np.mean(scores), np.std(scores)))
        all_scores[name] = scores





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    one-hot encoding
    r2 score:  mean: 0.790; std: 0.035

    similarity encoding
    r2 score:  mean: 0.930; std: 0.011

    minhash encoding
    r2 score:  mean: 0.924; std: 0.012

    gap encoding
    r2 score:  mean: 0.932; std: 0.009





.. GENERATED FROM PYTHON SOURCE LINES 343-346

Plotting the results
.....................
Finally, we plot the scores on a boxplot:

.. GENERATED FROM PYTHON SOURCE LINES 346-356

.. code-block:: Python


    import seaborn
    import matplotlib.pyplot as plt
    plt.figure(figsize=(4, 3))
    ax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')
    plt.ylabel('Encoding', size=20)
    plt.xlabel('Prediction accuracy     ', size=20)
    plt.yticks(size=20)
    plt.tight_layout()




.. image-sg:: /gen_notes/images/sphx_glr_02_dirty_categories_002.png
   :alt: 02 dirty categories
   :srcset: /gen_notes/images/sphx_glr_02_dirty_categories_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 357-368

The clear trend is that encoders that use the string form
of the category (similarity, minhash, and gap) perform better than
those that discard it.

SimilarityEncoder is the best performer, but it is less scalable on big
data than MinHashEncoder and GapEncoder. The most scalable encoder is
the MinHashEncoder. GapEncoder, on the other hand, has the benefit that
it provides interpretable features (see :ref:`sphx_glr_auto_examples_04_feature_interpretation_gap_encoder.py`)

|


.. GENERATED FROM PYTHON SOURCE LINES 368-376

.. code-block:: Python


    #
    # .. topic:: The TableVectorizer automates preprocessing
    #
    #   As this notebook demonstrates, many preprocessing steps can be
    #   automated by the |SV|, and the resulting pipeline can still be
    #   inspected, even with non-normalized entries.
    #








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (2 minutes 23.713 seconds)


.. _sphx_glr_download_gen_notes_02_dirty_categories.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/dirty-data-science/python/gh-pages?filepath=notes/gen_notes/02_dirty_categories.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: lite-badge

      .. image:: images/jupyterlite_badge_logo.svg
        :target: ../lite/retro/notebooks/?path=gen_notes/02_dirty_categories.ipynb
        :alt: Launch JupyterLite
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: 02_dirty_categories.ipynb <02_dirty_categories.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: 02_dirty_categories.py <02_dirty_categories.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
