
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gen_notes/missing_values.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gen_notes_missing_values.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gen_notes_missing_values.py:


=========================================
Machine learning with missing values
=========================================

Here we use simulated data to understanding the fundamentals of statistical
learning with missing values.

This notebook reveals why a HistGradientBoostingRegressor (
:class:`sklearn.ensemble.HistGradientBoostingRegressor` ) is a choice to
predict with missing values.

Simulations are very useful to control the missing-value mechanism, and
inspect it's impact on predictive models. In particular, standard
imputation procedures can reconstruct missing values with distortion only
if the data is *missing at random*.

The mathematical details behind this notebook can be found in
https://arxiv.org/abs/1902.06931

.. GENERATED FROM PYTHON SOURCE LINES 24-33

The fully-observed data: a toy regression problem
==================================================

We consider a simple regression problem where X (the data) is bivariate
gaussian, and y (the prediction target)  is a linear function of the first
coordinate, with noise.

The data-generating mechanism
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 33-48

.. code-block:: default


    import numpy as np

    def generate_without_missing_values(n_samples, rng=42):
        mean = [0, 0]
        cov = [[1, 0.5], [0.5, 1]]
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)
        X = rng.multivariate_normal(mean, cov, size=n_samples)

        epsilon = 0.1 * rng.randn(n_samples)
        y = X[:, 0] + epsilon

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 49-50

A quick plot reveals what the data looks like

.. GENERATED FROM PYTHON SOURCE LINES 50-59

.. code-block:: default


    import matplotlib.pyplot as plt
    plt.rcParams['figure.figsize'] = (5, 4) # Smaller default figure size

    plt.figure()
    X_full, y_full = generate_without_missing_values(1000)
    plt.scatter(X_full[:, 0], X_full[:, 1], c=y_full)
    plt.colorbar(label='y')




.. image:: /gen_notes/images/sphx_glr_missing_values_001.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7f0f33a0f7c0>



.. GENERATED FROM PYTHON SOURCE LINES 60-69

Missing completely at random settings
======================================

We now consider missing completely at random settings (a special case
of missing at random): the missingness is completely independent from
the values.

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 69-80

.. code-block:: default


    def generate_mcar(n_samples, missing_rate=.5, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        M = rng.binomial(1, missing_rate, (n_samples, 2))
        np.putmask(X, M, np.nan)

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 81-82

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 82-90

.. code-block:: default

    X, y = generate_mcar(500)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5', label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_missing_values_002.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f0f3386fbe0>



.. GENERATED FROM PYTHON SOURCE LINES 91-102

We can see that the distribution of the fully-observed data is the same
than that of the original data

Conditional Imputation with the IterativeImputer
------------------------------------------------

As the data is MAR (missing at random), an imputer can use the
conditional dependencies between the observed and the missing values to
impute the missing values.

We'll use the IterativeImputer, a good imputer, but it needs to be enabled

.. GENERATED FROM PYTHON SOURCE LINES 102-106

.. code-block:: default

    from sklearn.experimental import enable_iterative_imputer
    from sklearn import impute
    iterative_imputer = impute.IterativeImputer()








.. GENERATED FROM PYTHON SOURCE LINES 107-111

Let us try the imputer on the small data used to visualize

**The imputation is learned by fitting the imputer on the data with
missing values**

.. GENERATED FROM PYTHON SOURCE LINES 111-113

.. code-block:: default

    iterative_imputer.fit(X)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    IterativeImputer()



.. GENERATED FROM PYTHON SOURCE LINES 114-115

**The data are imputed with the transform method**

.. GENERATED FROM PYTHON SOURCE LINES 115-117

.. code-block:: default

    X_imputed = iterative_imputer.transform(X)








.. GENERATED FROM PYTHON SOURCE LINES 118-119

We can display the imputed data as our previous visualization

.. GENERATED FROM PYTHON SOURCE LINES 119-127

.. code-block:: default

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data', alpha=.5)
    plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',
                label='Imputed')
    plt.colorbar(label='y')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_missing_values_003.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f0f158de490>



.. GENERATED FROM PYTHON SOURCE LINES 128-140

We can see that the imputer did a fairly good job of recovering the
data distribution

Supervised learning with a linear model
----------------------------------------

Given that the relationship between the fully-observed X and y is a
linear relationship, it seems natural to use a linear model for
prediction, which must be adapted to missing values using imputation.

To use it in supervised setting, we will pipeline it with a linear
model, using a ridge, which is a good default linear model

.. GENERATED FROM PYTHON SOURCE LINES 140-145

.. code-block:: default

    from sklearn.pipeline import make_pipeline
    from sklearn.linear_model import RidgeCV

    iterative_and_ridge = make_pipeline(impute.IterativeImputer(), RidgeCV())








.. GENERATED FROM PYTHON SOURCE LINES 146-149

We can evaluate the model performance in a cross-validation loop
(for better evaluation accuracy, we increase slightly the number of
folds to 10)

.. GENERATED FROM PYTHON SOURCE LINES 149-155

.. code-block:: default

    from sklearn import model_selection
    scores_iterative_and_ridge = model_selection.cross_val_score(
        iterative_and_ridge, X, y, cv=10)

    scores_iterative_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.46386365, 0.31905505, 0.62796809, 0.50172353, 0.5526797 ,
           0.52067976, 0.66790152, 0.71185873, 0.52386641, 0.45218862])



.. GENERATED FROM PYTHON SOURCE LINES 156-159

**Computational cost** One drawback of the IterativeImputer to keep in
mind is that its computational cost can become prohibitive of large
datasets (it has a bad computation scalability).

.. GENERATED FROM PYTHON SOURCE LINES 161-165

Mean imputation: SimpleImputer
-------------------------------

We can try a simple imputer: imputation by the mean

.. GENERATED FROM PYTHON SOURCE LINES 165-167

.. code-block:: default

    mean_imputer = impute.SimpleImputer()








.. GENERATED FROM PYTHON SOURCE LINES 168-169

A quick visualization reveals a larger disortion of the distribution

.. GENERATED FROM PYTHON SOURCE LINES 169-177

.. code-block:: default

    X_imputed = mean_imputer.fit_transform(X)
    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data', alpha=.5)
    plt.scatter(X_imputed[:, 0], X_imputed[:, 1], c=y, marker='X',
                label='Imputed')
    plt.colorbar(label='y')




.. image:: /gen_notes/images/sphx_glr_missing_values_004.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7f0f1586d460>



.. GENERATED FROM PYTHON SOURCE LINES 178-179

Evaluating in prediction pipeline

.. GENERATED FROM PYTHON SOURCE LINES 179-185

.. code-block:: default

    mean_and_ridge = make_pipeline(impute.SimpleImputer(), RidgeCV())
    scores_mean_and_ridge = model_selection.cross_val_score(
        mean_and_ridge, X, y, cv=10)

    scores_mean_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.54614659, 0.43926104, 0.59187943, 0.44029222, 0.53507246,
           0.41132337, 0.67374847, 0.79018856, 0.48898176, 0.46854395])



.. GENERATED FROM PYTHON SOURCE LINES 186-191

Supervised learning without imputation
----------------------------------------

The HistGradientBoosting models are based on trees, which can be
adapted to model directly missing values

.. GENERATED FROM PYTHON SOURCE LINES 191-198

.. code-block:: default

    from sklearn.experimental import enable_hist_gradient_boosting
    from sklearn.ensemble import HistGradientBoostingRegressor
    score_hist_gradient_boosting = model_selection.cross_val_score(
        HistGradientBoostingRegressor(), X, y, cv=10)

    score_hist_gradient_boosting





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/varoquau/dev/scikit-learn/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.
      warnings.warn(

    array([0.48104853, 0.3676542 , 0.54779672, 0.43052366, 0.53230889,
           0.31475748, 0.66549077, 0.70964002, 0.5001503 , 0.43511816])



.. GENERATED FROM PYTHON SOURCE LINES 199-203

Recap: which pipeline predicts well on our small data?
-------------------------------------------------------

Let's plot the scores to see things better

.. GENERATED FROM PYTHON SOURCE LINES 203-217

.. code-block:: default

    import pandas as pd
    import seaborn as sns

    scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,
                 'IterativeImputer + Ridge': scores_iterative_and_ridge,
                 'HistGradientBoostingRegressor': score_hist_gradient_boosting,
        })

    sns.boxplot(data=scores, orient='h')
    plt.title('Prediction accuracy\n linear and small data\n'
              'Missing Completely at Random')
    plt.tight_layout()





.. image:: /gen_notes/images/sphx_glr_missing_values_005.png
    :alt: Prediction accuracy  linear and small data Missing Completely at Random
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 218-225

Not much difference with the more sophisticated imputer. A more thorough
analysis would be necessary, with more cross-validation runs.

Prediction performance with large datasets
-------------------------------------------

Let us compare models in regimes where there is plenty of data

.. GENERATED FROM PYTHON SOURCE LINES 225-228

.. code-block:: default


    X, y = generate_mcar(n_samples=20000)








.. GENERATED FROM PYTHON SOURCE LINES 229-230

Iterative imputation and linear model

.. GENERATED FROM PYTHON SOURCE LINES 230-234

.. code-block:: default

    scores_iterative_and_ridge= model_selection.cross_val_score(
        iterative_and_ridge, X, y, cv=10)
    scores_iterative_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.56285628, 0.56629022, 0.54287485, 0.54440354, 0.5131173 ,
           0.53812876, 0.53808635, 0.59017208, 0.52860526, 0.59019199])



.. GENERATED FROM PYTHON SOURCE LINES 235-236

Mean imputation and linear model

.. GENERATED FROM PYTHON SOURCE LINES 236-240

.. code-block:: default

    scores_mean_and_ridge = model_selection.cross_val_score(
        mean_and_ridge, X, y, cv=10)
    scores_mean_and_ridge





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.54044989, 0.53588731, 0.52461719, 0.52562038, 0.50379387,
           0.51241996, 0.54008036, 0.57123571, 0.5225978 , 0.5729993 ])



.. GENERATED FROM PYTHON SOURCE LINES 241-243

And now the HistGradientBoostingRegressor, which does not need
imputation

.. GENERATED FROM PYTHON SOURCE LINES 243-247

.. code-block:: default

    score_hist_gradient_boosting = model_selection.cross_val_score(
        HistGradientBoostingRegressor(), X, y, cv=10)
    score_hist_gradient_boosting





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    array([0.57156313, 0.57063407, 0.55248645, 0.552128  , 0.52246108,
           0.54209931, 0.55694803, 0.60000661, 0.54179656, 0.60075522])



.. GENERATED FROM PYTHON SOURCE LINES 248-249

We plot the results

.. GENERATED FROM PYTHON SOURCE LINES 249-260

.. code-block:: default

    scores = pd.DataFrame({'Mean imputation + Ridge': scores_mean_and_ridge,
                 'IterativeImputer + Ridge': scores_iterative_and_ridge,
                 'HistGradientBoostingRegressor': score_hist_gradient_boosting,
        })

    sns.boxplot(data=scores, orient='h')
    plt.title('Prediction accuracy\n linear and large data\n'
              'Missing Completely at Random')
    plt.tight_layout()





.. image:: /gen_notes/images/sphx_glr_missing_values_006.png
    :alt: Prediction accuracy  linear and large data Missing Completely at Random
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 261-270

Missing not at random: censoring
======================================

We now consider missing not at random settings, in particular
self-masking or censoring, where large values are more likely to be
missing.

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 270-283

.. code-block:: default


    def generate_censored(n_samples, missing_rate=0.2, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        B = np.random.binomial(1, 2 * missing_rate, (n_samples, 2))
        M = (X > 0.5) * B

        np.putmask(X, M, np.nan)

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 284-285

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 285-294

.. code-block:: default

    X, y = generate_censored(500, missing_rate=.4)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_missing_values_007.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f0f10d9da00>



.. GENERATED FROM PYTHON SOURCE LINES 295-297

Here the full-observed data does not reflect well at all the
distribution of all the data

.. GENERATED FROM PYTHON SOURCE LINES 299-304

Using a predictor for the fully-observed case
==============================================

Let us go back to the "easy" case of the missing completely at random
settings with plenty of data

.. GENERATED FROM PYTHON SOURCE LINES 304-308

.. code-block:: default

    n_samples = 10000

    X, y = generate_mcar(n_samples, missing_rate=.5)








.. GENERATED FROM PYTHON SOURCE LINES 309-311

Suppose we have been able to train a predictive model that works on
fully-observed data:

.. GENERATED FROM PYTHON SOURCE LINES 311-315

.. code-block:: default


    X_full, y_full = generate_without_missing_values(n_samples)










.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  4.748 seconds)


.. _sphx_glr_download_gen_notes_missing_values.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/dirty-data-science/python/gh-pages?filepath=notes/gen_notes/missing_values.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: missing_values.py <missing_values.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: missing_values.ipynb <missing_values.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
