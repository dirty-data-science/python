
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gen_notes/missing_values.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gen_notes_missing_values.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gen_notes_missing_values.py:


=========================================
Machine learning with missing values
=========================================

Here we use simulated data to understanding the fundamentals of statistical
learning with missing values.

This notebook reveals why a HistGradientBoostingRegressor (
:class:`sklearn.ensemble.HistGradientBoostingRegressor` ) is a choice to
predict with missing values.

.. GENERATED FROM PYTHON SOURCE LINES 13-19

.. code-block:: default


    import numpy as np

    import matplotlib.pyplot as plt
    plt.rcParams['figure.figsize'] = (5, 4) # Smaller default figure size








.. GENERATED FROM PYTHON SOURCE LINES 20-29

The fully-observed data: a toy regression problem
==================================================

We consider a simple regression problem where X (the data) is bivariate
gaussian, and y (the prediction target)  is a linear function of the first
coordinate, with noise.

The data-generating mechanism
------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 29-42

.. code-block:: default


    def generate_without_missing_values(n_samples, rng=42):
        mean = [0, 0]
        cov = [[1, 0.5], [0.5, 1]]
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)
        X = rng.multivariate_normal(mean, cov, size=n_samples)

        epsilon = 0.1 * rng.randn(n_samples)
        y = X[:, 0] + epsilon

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 43-44

A quick plot reveals what the data looks like

.. GENERATED FROM PYTHON SOURCE LINES 44-50

.. code-block:: default


    plt.figure()
    X_full, y_full = generate_without_missing_values(500)
    plt.scatter(X_full[:, 0], X_full[:, 1], c=y_full)
    plt.colorbar(label='y')




.. image:: /gen_notes/images/sphx_glr_missing_values_001.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.colorbar.Colorbar object at 0x7fa47d221550>



.. GENERATED FROM PYTHON SOURCE LINES 51-59

Missing completely at random setting
======================================

We now consider missing completely at random settings (a special case
of missing at random).

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 59-70

.. code-block:: default


    def generate_mcar(n_samples, missing_rate=0.2, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        M = rng.binomial(1, missing_rate, (n_samples, 2))
        np.putmask(X, M, np.nan)

        return X, y








.. GENERATED FROM PYTHON SOURCE LINES 71-72

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 72-81

.. code-block:: default

    X, y = generate_mcar(500, missing_rate=.5)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()




.. image:: /gen_notes/images/sphx_glr_missing_values_002.png
    :alt: missing values
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7fa47d07cc40>



.. GENERATED FROM PYTHON SOURCE LINES 82-91

We can see that the distribution of the fully-observed data is the same
than that of the original data

Building a predictive model: imputation and simple model
--------------------------------------------------------

Given that the relationship between the fully-observed X and y is a
linear relationship, it seems natural to use a linear model for
prediction, which must be adapted to missing values using imputation.

.. GENERATED FROM PYTHON SOURCE LINES 91-95

.. code-block:: default


    from sklearn.linear_model import RidgeCV # Good default linear model
    from sklearn.impute import IterativeImputive # Good imputer



.. rst-class:: sphx-glr-script-out

.. code-block:: pytb

    Traceback (most recent call last):
      File "/home/varoquau/dev/dirty_data_tutorial/notes/missing_values.py", line 93, in <module>
        from sklearn.impute import IterativeImputive # Good imputer
    ImportError: cannot import name 'IterativeImputive' from 'sklearn.impute' (/home/varoquau/dev/scikit-learn/sklearn/impute/__init__.py)




.. GENERATED FROM PYTHON SOURCE LINES 96-105

Missing not at random: censoring
======================================

We now consider missing not at random settings, in particular
self-masking or censoring, where large values are more likely to be
missing.

The missing-values mechanism
-----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 105-118

.. code-block:: default


    def generate_censored(n_samples, missing_rate=0.2, rng=42):
        X, y = generate_without_missing_values(n_samples, rng=rng)
        if not isinstance(rng, np.random.RandomState):
            rng = np.random.RandomState(rng)

        B = np.random.binomial(1, 2 * missing_rate, (n_samples, 2))
        M = (X > 0.5) * B

        np.putmask(X, M, np.nan)

        return X, y


.. GENERATED FROM PYTHON SOURCE LINES 119-120

A quick plot to look at the data

.. GENERATED FROM PYTHON SOURCE LINES 120-129

.. code-block:: default

    X, y = generate_censored(500, missing_rate=.4)

    plt.figure()
    plt.scatter(X_full[:, 0], X_full[:, 1], color='.8', ec='.5',
                label='All data')
    plt.colorbar(label='y')
    plt.scatter(X[:, 0], X[:, 1], c=y, label='Fully observed')
    plt.legend()


.. GENERATED FROM PYTHON SOURCE LINES 130-132

Here the full-observed data does not reflect well at all the
distribution of all the data

.. GENERATED FROM PYTHON SOURCE LINES 134-139

Using a predictor for the fully-observed case
==============================================

Let us go back to the "easy" case of the missing completely at random
settings with plenty of data

.. GENERATED FROM PYTHON SOURCE LINES 139-143

.. code-block:: default

    n_samples = 10000

    X, y = generate_mcar(n_samples, missing_rate=.5)


.. GENERATED FROM PYTHON SOURCE LINES 144-146

Suppose we have been able to train a predictive model that works on
fully-observed data:

.. GENERATED FROM PYTHON SOURCE LINES 146-150

.. code-block:: default


    X_full, y_full = generate_without_missing_values(n_samples)




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.574 seconds)


.. _sphx_glr_download_gen_notes_missing_values.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/dirty-data-science/python/gh-pages?filepath=notes/gen_notes/missing_values.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: missing_values.py <missing_values.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: missing_values.ipynb <missing_values.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
